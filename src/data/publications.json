[
    {
        "id": "jeon2026enhancing",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    9
                ]
            ]
        },
        "author": "Jeon, Hyun-Sik, Kang, Seong-Hui, Ha, Jong-Eun",
        "citation-key": "jeon2026enhancing",
        "container-title": "Applied Sciences",
        "DOI": "10.3390/app16020652",
        "ISSN": "2076-3417",
        "issue": "2",
        "issued": {
            "date-parts": [
                [
                    "2026",
                    1
                ]
            ]
        },
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "page": "652",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "source": "www.mdpi.com",
        "title": "Enhancing SeeGround with Relational Depth Text for 3D Visual Grounding",
        "type": "article-journal",
        "URL": "https://www.mdpi.com/2076-3417/16/2/652",
        "volume": "16",
        "category": "international-journal",
        "year": 2026,
        "venue": "Applied Sciences",
        "language": "En",
        "organizations": [
            {
                "name": "Applied Sciences",
                "url": "https://www.mdpi.com/journal/applsci"
            }
        ],
        "doi": "10.3390/app16020652",
        "abstract": "• **Problem**: Traditional 3D visual grounding (e.g., SeeGround) struggles with distinguishing objects based on relative depth due to implicit depth representation in 2D views.\n• **Solution**: Proposed **Relational Depth Text (RDT)**, which explicitly converts relative depth relationships (e.g., 'behind', 'in front of') into natural language using Monocular Depth Estimation and K-NN.\n• **Result**: Achieved **3.54% accuracy improvement** on Nr3D and **6.74% boost** in Unique cases on ScanRefer, enhancing spatial reasoning in complex 3D scenes.",
        "media": [
            {
                "type": "image",
                "url": "/assets/project/SeeGround_Fig_1.webp",
                "caption": "The inference pipeline is integrated with the proposed ‘Relational Depth Text’ methodology."
            }
        ],
        "table": {
            "caption": "Comparison with state-of-the-art zero-shot methods on Nr3D and ScanRefer benchmarks. (Unit: %).",
            "columns": [
                "Method",
                "Backbone (Size)",
                "Nr3D (Acc@25)",
                "ScanRefer (Acc@25)",
                "ScanRefer (Acc@50)"
            ],
            "rows": [
                [
                    "VLM-Grounder",
                    "GPT-4V (Large)",
                    "48.00 †",
                    "51.60 †",
                    "32.80 †"
                ],
                [
                    "SORT3D",
                    "GPT-4o (Large)",
                    "62.00 †",
                    "-",
                    "-"
                ],
                [
                    "SeqVLM",
                    "Doubao-1.5-vision-pro",
                    "53.20 †",
                    "55.60 †",
                    "49.60 †"
                ],
                [
                    "View-on-Graph",
                    "Qwen2-VL-72B",
                    "47.60 †",
                    "44.80 †",
                    "40.30 †"
                ],
                [
                    "SeeGround (Baseline)",
                    "Qwen2-VL-7B",
                    "33.18",
                    "37.59",
                    "35.04"
                ],
                [
                    "Ours 7B+Depth",
                    "Qwen2-VL-7B",
                    "36.72",
                    "38.01",
                    "34.33"
                ]
            ],
            "highlightRowIndex": 5
        },
        "resultImages": [
            {
                "url": "/assets/project/SeeGround_Fig_2.webp",
                "caption": "Qualitative comparison results on the Nr3D dataset."
            },
            {
                "url": "/assets/project/SeeGround_Fig_3.webp",
                "caption": "Qualitative comparison results on the ScanRefer dataset."
            }
        ]
    },
    {
        "id": "jeonhyeonsig2025uniad",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    5
                ]
            ]
        },
        "author": "전현식, 박상민, 하종은",
        "citation-key": "jeonhyeonsig2025uniad",
        "container-title": "제어로봇시스템학회 논문지",
        "DOI": "10.5302/J.ICROS.2025.24.0249",
        "ISSN": "1976-5622",
        "issue": "4",
        "issued": {
            "date-parts": [
                [
                    "2025",
                    4
                ]
            ]
        },
        "page": "256–264",
        "source": "www.dbpia.co.kr",
        "title": "UniAD Model Lightweighting and Performance Comparison",
        "type": "article-journal",
        "URL": "https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE12125019",
        "volume": "31",
        "category": "domestic-journal",
        "year": 2025,
        "venue": "ICROS",
        "language": "Ko",
        "organizations": [
            {
                "name": "ICROS",
                "url": "https://icros.org/eng/"
            }
        ],
        "doi": "10.5302/J.ICROS.2025.24.0249",
        "abstract": "• **Problem**: The original UniAD framework's **complex transformer structure requires substantial computational resources**, limiting accessibility.\n• **Solution**: Proposed a **lightweight version** by reducing dimensions, queries, and BEV resolution, and **optimized memory** through limited sampling queries and page-locked settings.\n• **Result**: Achieved **significant memory reductions** (up to 79.92% in Stage 1) and identified that **progressive resolution expansion** enhances feature extraction during low-resolution phases.",
        "media": [
            {
                "type": "image",
                "url": "/assets/project/UniAD_Fig_1.webp",
                "caption": "The structure of UniAD (Unified Autonomous Driving)"
            }
        ],
        "tables": [
            {
                "caption": "Comparison of structures by lightweight version (the two values in the table represent the structures in stages 1 and 2, respectively).",
                "headerRows": [
                    [
                        {
                            "label": "Method"
                        },
                        {
                            "label": "GPU*N (NVIDIA)"
                        },
                        {
                            "label": "Encoder",
                            "rotated": true
                        },
                        {
                            "label": "embed dimensions",
                            "rotated": true
                        },
                        {
                            "label": "layers",
                            "rotated": true
                        },
                        {
                            "label": "heads",
                            "rotated": true
                        },
                        {
                            "label": "FFN channels",
                            "rotated": true
                        },
                        {
                            "label": "BEV resolution",
                            "rotated": true
                        },
                        {
                            "label": "Queue length (stage1)",
                            "rotated": true
                        },
                        {
                            "label": "BEV queries",
                            "rotated": true
                        },
                        {
                            "label": "Seg queries",
                            "rotated": true
                        },
                        {
                            "label": "Learning Rate",
                            "rotated": true
                        },
                        {
                            "label": "Grid Size",
                            "rotated": true
                        },
                        {
                            "label": "Query dimensions (stage2)",
                            "rotated": true
                        },
                        {
                            "label": "Pin memory",
                            "rotated": true
                        },
                        {
                            "label": "Memory threshold",
                            "rotated": true
                        }
                    ]
                ],
                "rows": [
                    [
                        "UniAD",
                        "A100*16",
                        "R101",
                        "256",
                        "6",
                        "8",
                        "2048",
                        "200x200",
                        "5",
                        "900",
                        "300",
                        "2e-4",
                        "512",
                        "256",
                        "False",
                        "3"
                    ],
                    [
                        "DV-1",
                        "3090*3",
                        "R101",
                        "128",
                        "4",
                        "8",
                        "1024",
                        "200x200",
                        "3",
                        "900",
                        "300",
                        "1e-6 / 1e-4",
                        "512 / 256",
                        "128",
                        "False",
                        "3"
                    ],
                    [
                        "DV-2",
                        "A100*1",
                        "R50",
                        "128",
                        "4",
                        "4",
                        "1024",
                        "50x50 / 200",
                        "3",
                        "900",
                        "300",
                        "2e-4",
                        "512",
                        "128",
                        "True",
                        "1"
                    ]
                ],
                "highlightRowIndex": 2
            },
            {
                "caption": "Comparison of experimental results by lightweight version (the two values in the Memory section represent stages 1 and 2, respectively).",
                "headerRows": [
                    [
                        {
                            "label": "Method",
                            "rowSpan": 2
                        },
                        {
                            "label": "Memory (GB)",
                            "rowSpan": 2
                        },
                        {
                            "label": "Tracking",
                            "colSpan": 3
                        },
                        {
                            "label": "Mapping",
                            "colSpan": 2
                        },
                        {
                            "label": "Motion Forecasting",
                            "colSpan": 4
                        },
                        {
                            "label": "Occupancy Prediction",
                            "colSpan": 4
                        },
                        {
                            "label": "Planning",
                            "colSpan": 2
                        }
                    ],
                    [
                        {
                            "label": "AMOTA ↑"
                        },
                        {
                            "label": "AMOTP ↓"
                        },
                        {
                            "label": "mIDS ↓"
                        },
                        {
                            "label": "IoU-lane ↑"
                        },
                        {
                            "label": "IoU-road ↑"
                        },
                        {
                            "label": "minADE ↓"
                        },
                        {
                            "label": "minFDE ↓"
                        },
                        {
                            "label": "MR ↓"
                        },
                        {
                            "label": "EPA ↑"
                        },
                        {
                            "label": "IoU-n. ↑"
                        },
                        {
                            "label": "IoU-f. ↑"
                        },
                        {
                            "label": "VPQ-n. ↑"
                        },
                        {
                            "label": "VPQ-f. ↑"
                        },
                        {
                            "label": "avg.L2 ↓"
                        },
                        {
                            "label": "avg.Col. ↓"
                        }
                    ]
                ],
                "rows": [
                    [
                        "UniAD(Base)",
                        "52.3 / 16.7",
                        "**0.359**",
                        "**1.32**",
                        "906",
                        "0.313",
                        "**0.691**",
                        "**0.708**",
                        "**1.025**",
                        "0.151",
                        "**0.456**",
                        "**63.4**",
                        "**40.2**",
                        "**54.7**",
                        "**33.5**",
                        "1.03",
                        "**0.31**"
                    ],
                    [
                        "UniAD(Small)",
                        "N/A",
                        "0.241",
                        "1.488",
                        "958",
                        "**0.315**",
                        "0.689",
                        "0.788",
                        "1.126",
                        "0.156",
                        "0.381",
                        "59.4",
                        "35.6",
                        "49.2",
                        "28.9",
                        "1.04",
                        "0.32"
                    ],
                    [
                        "DV-1",
                        "17.9 / 10.4",
                        "0.000",
                        "1.946",
                        "1493",
                        "0.253",
                        "0.615",
                        "<u>0.995</u>",
                        "<u>1.274</u>",
                        "<u>**0.130**</u>",
                        "-0.077",
                        "25.7",
                        "9.8",
                        "15.4",
                        "5.7",
                        "1.25",
                        "0.88"
                    ],
                    [
                        "DV-2",
                        "10.5 / 10.2",
                        "<u>0.016</u>",
                        "<u>1.816</u>",
                        "<u>**753**</u>",
                        "<u>0.304</u>",
                        "<u>0.684</u>",
                        "1.157",
                        "1.658",
                        "0.208",
                        "<u>0.108</u>",
                        "<u>43.0</u>",
                        "<u>22.0</u>",
                        "<u>29.6</u>",
                        "<u>15.0</u>",
                        "<u>**0.97**</u>",
                        "<u>0.75</u>"
                    ]
                ],
                "highlightRowIndex": 3
            }
        ],
        "resultImages": [
            {
                "url": "/assets/project/UniAD_Fig_7_8_9.webp",
                "caption": "Test results by model: Urban driving scenario. (a) UniAD. (b) DV-1. (c) DV-2."
            },
            {
                "url": "/assets/project/UniAD_Fig_10_11_12.webp",
                "caption": "Test results by model: Pedestrian crossing scenario. (a) UniAD. (b) DV-1. (c) DV-2."
            }
        ]
    },
    {
        "id": "jeonhyeonsig2025viewformer",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    5
                ]
            ]
        },
        "author": "전현식, 김유민, 하종은",
        "citation-key": "jeonhyeonsig2025viewformer",
        "container-title": "제어로봇시스템학회 논문지",
        "DOI": "10.5302/J.ICROS.2025.25.0168",
        "ISSN": "1976-5622",
        "issue": "10",
        "issued": {
            "date-parts": [
                [
                    "2025",
                    10
                ]
            ]
        },
        "page": "1160–1168",
        "source": "www.dbpia.co.kr",
        "title": "Object Mask Module for Enhancing Multi-view 3D Occupancy Perception Performance Based on ViewFormer",
        "type": "article-journal",
        "URL": "https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE12406310",
        "volume": "31",
        "category": "domestic-journal",
        "year": 2025,
        "venue": "ICROS",
        "language": "Ko",
        "organizations": [
            {
                "name": "ICROS",
                "url": "https://icros.org/eng/"
            }
        ],
        "doi": "10.5302/J.ICROS.2025.25.0168",
        "abstract": "• **Problem**: ViewFormer effectively captures spatiotemporal information but **underperforms on small objects** such as pedestrians and bicycles.\n• **Solution**: Designed a **SegFormer-based object masking module** that estimates object probabilities from BEV features and concatenates them as an additional feature channel.\n• **Result**: Experimental evaluations on the nuScenes dataset revealed an **unexpected performance decline** in overall metrics, and subsequent analysis indicated **weak mask activation and instability** during initial training, highlighting the need for **structural adjustments and improved training strategies** in future work.",
        "media": [
            {
                "type": "image",
                "url": "/assets/project/Viewformer_Fig_1.webp",
                "caption": "Overall architecture of ViewFormer with the proposed Object Mask module."
            }
        ],
        "tables": [
            {
                "caption": "Comparison of model performance.",
                "columns": [
                    "Method",
                    "Training GPU",
                    "Test GPU",
                    "Training Time",
                    "Memory(G)",
                    "FPS",
                    "FLOPs",
                    "Params",
                    "IoUgeo",
                    "mIoU"
                ],
                "rows": [
                    [
                        "ViewFormer",
                        "4090*2",
                        "4090*2",
                        "5D 22h",
                        "13.7",
                        "12.2",
                        "214.74",
                        "99.19 M",
                        "71.03",
                        "41.37"
                    ],
                    [
                        "Try 1 (126⊙126) CNN",
                        "4090*2",
                        "4090*2",
                        "7D 7h",
                        "14.1",
                        "6.2",
                        "242.14",
                        "99.28 M",
                        "62.48",
                        "25.45"
                    ],
                    [
                        "Try 2 (126⊕1) CNN",
                        "4090*2",
                        "4090*2",
                        "6D 23h",
                        "13.7",
                        "12.1",
                        "242.14",
                        "99.28 M",
                        "62.76",
                        "25.73"
                    ],
                    [
                        "Ours (126⊕1) Seg",
                        "4090*2",
                        "4090*2",
                        "9D 5h",
                        "16.2",
                        "10.7",
                        "249.30",
                        "181.21 M",
                        "64.89",
                        "28.76"
                    ]
                ],
                "highlightRowIndex": 3
            },
            {
                "caption": "Early performance comparison when training the Seg model on a single GPU.",
                "columns": [
                    "Method",
                    "Training GPU",
                    "Test GPU",
                    "Training Time",
                    "Memory(G)",
                    "FPS",
                    "FLOPs",
                    "Params",
                    "IoUgeo",
                    "mIoU"
                ],
                "rows": [
                    [
                        "ViewFormer",
                        "3090*1",
                        "1/10",
                        "22D",
                        "14.6",
                        "-",
                        "-",
                        "99.19 M",
                        "62.88",
                        "27.36"
                    ],
                    [
                        "Ours - single (126⊕1) Seg",
                        "3090*1",
                        "1/10",
                        "32D",
                        "14.9",
                        "-",
                        "-",
                        "181.21 M",
                        "62.53",
                        "28.28"
                    ]
                ],
                "highlightRowIndex": 1
            }
        ],
        "resultImages": [
            {
                "url": "/assets/project/Viewformer_Fig_2.webp",
                "caption": "Ground truth mask for object mask learning."
            },
            {
                "url": "/assets/project/Viewformer_Fig_3.webp",
                "caption": "Visualization of BEV feature changes caused by the application of object masks across different stages of training."
            }
        ]
    },
    {
        "id": "ijaeug2023lidar",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    15
                ]
            ]
        },
        "author": "이재욱, 전현식, 최용수",
        "citation-key": "ijaeug2023lidar",
        "container-title": "ITFE Conference",
        "issued": {
            "date-parts": [
                [
                    "2023",
                    8
                ]
            ]
        },
        "page": "227–228",
        "source": "www.earticle.net",
        "title": "LiDAR 센서 기반 저사양 환경 자율주행 플랫폼의 트래픽 콘 사이 조향-속도 제어",
        "type": "article-journal",
        "URL": "https://www.earticle.net/Article/A435695",
        "category": "domestic-journal",
        "year": 2023,
        "venue": "ITFE",
        "language": "Ko",
        "organizations": [
            {
                "name": "ITFE",
                "url": "https://www.itfe.or.kr/html/"
            }
        ]
    },
    {
        "id": "ijaeug2023lidara",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    15
                ]
            ]
        },
        "author": "이재욱, 전현식, 최용수",
        "citation-key": "ijaeug2023lidara",
        "container-title": "대한전자공학회 학술대회",
        "issued": {
            "date-parts": [
                [
                    "2023",
                    6
                ]
            ]
        },
        "page": "2434–2436",
        "source": "www.dbpia.co.kr",
        "title": "LiDAR 센서를 활용한 저사양 환경 자율주행",
        "type": "article-journal",
        "URL": "https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11522688",
        "category": "domestic-journal",
        "year": 2023,
        "venue": "IEIE",
        "language": "Ko",
        "organizations": [
            {
                "name": "IEIE",
                "url": "https://www.theieie.org/eng/"
            }
        ]
    },
    {
        "id": "anseongju2022jeogeungjeog",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    15
                ]
            ]
        },
        "author": "안성주, 김성택, 전현식, 최용수",
        "citation-key": "anseongju2022jeogeungjeog",
        "container-title": "ITFE Conference",
        "issued": {
            "date-parts": [
                [
                    "2022",
                    11
                ]
            ]
        },
        "page": "24–25",
        "source": "www.earticle.net",
        "title": "적응적 차량요소 제거를 통한 차선인식 향상방법",
        "type": "article-journal",
        "URL": "https://www.earticle.net/Article/A427683",
        "category": "domestic-journal",
        "year": 2022,
        "venue": "ITFE",
        "language": "Ko",
        "organizations": [
            {
                "name": "ITFE",
                "url": "https://www.itfe.or.kr/html/"
            }
        ]
    },
    {
        "id": "jeonhyeonsig2022caseon",
        "accessed": {
            "date-parts": [
                [
                    "2026",
                    1,
                    15
                ]
            ]
        },
        "author": "전현식, 안성주, 김성택, 최용수",
        "citation-key": "jeonhyeonsig2022caseon",
        "container-title": "대한전자공학회 학술대회",
        "issued": {
            "date-parts": [
                [
                    "2022",
                    11
                ]
            ]
        },
        "page": "80–82",
        "source": "www.dbpia.co.kr",
        "title": "차선 인식 향상을 위한 차량요소 제거 방법",
        "type": "article-journal",
        "URL": "https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11195427",
        "category": "domestic-journal",
        "year": 2022,
        "venue": "IEIE",
        "language": "Ko",
        "organizations": [
            {
                "name": "IEIE",
                "url": "https://www.theieie.org/eng/"
            }
        ]
    }
]